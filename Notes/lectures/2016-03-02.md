# Scheduling Part 1

### Review: Scheduling
CPU/IO burst cycles, and these are typically expected while the actual operations vary somewhat. Some of the criteria of performance metrics: CPU utilization, turn around time (time to complete the job from submission, though may not care for longer running processes), response time, etc. Scheduling algorithms help solve certain problems

### Job Characterizations
* Computational Jobs: typically long CPU bursts
* Interactive jobs: short CPU bursts (but not always long IO bursts)
* RealTime jobs: characterized by hard deadlines

### Mechanism vs Policy
* The idea is to separate mechanism (code and data that enables functionality) from policy (algorithm). 
* The cache is an example. It is a latency tolerant mechanism, but the policy for usage is a usage policy (like LRU). If you have a policy, then you have pros and cons. Tooling vs how you use the tool. LRU may not be useful for certain things like streaming (never guarenteed locality)
 
### Scheduling 
Jobs can change and fluctuate between IO intensive to CPU intensive. We also typically have multiple jobs. So we attempt to schedule CPU bursts (as IO is handling IO scheduling). So you can say either run to completion (non-pre-emptive) or run up to X time units (pre-emptive)
* __Time Quantum__ a unit of time over which something processes. So you can run until IO burst or your _quantum_ is up. 
  * When quantum time is small, then you have essentially processor sharing so it will seem like the procs are concurrent
* __Priority__ with pre-emption, higher prio processes will get to run before lower ones. When finishing a quantum/job, you always redecide what is ran again. 

### Scheduling Policies
__Shorted Job Next__ or schedule the shortest job next
* CPU utilization will be high/good (+)
* This is unfair to long running jobs since they are avoided while short jobs will preferred (but still may be fair if you will only have short jobs, but then it is relative)
* Response time
* Predictability (-) not that great since we have no idea when a job will run. 
  * This requires solving the halting problem in a general computer, so we can not predict when jobs will finish. So this isimpossible to implement.
* Throughput (+) This is actually optimal and is very high. 

__Round Robin__ Processes run until quantum time unit (typically nanoseconds) or until IO burst cycle/you finish/voluntarily yield. With small quantum time units (`q`), then you appear to have concurrency. Large `q` decomposes this to FCFS. Though with super tiny `q`, then the overhead incurred with switching may be counter productive. 
* Fairness: Copmutational processes consume majority of `q` while IO bursts consume extremely small amounts of `q`. So may be fine. 
* Response Time: Maximal response time is when next scheduled which requires knowledge of amount of processes in the queue * `q` + time to switch * length of queue. 
  * Fairly good unless you have a ton of processes (which is more indicative of the workload) 
* Predictable: (-) 
* Throughput: similar to FCFS, but you are now doing CPU sharing, so lower throughput. 
 
__Priority Queue__ Sort the queue by priority. So RR with multiple queues? Jobs at the head of the queue run and higher prio queues are preferred. High prio procs kick off low prio procs and run on CPUs. Starvation can happen however if the high prio queues always have a high prio proc. May not work for long running processes. 

__Priority Inversion__ Assume 2 procs with high and low prio. Low is in a critical section while high is in a busy wait for entry to the same critical section. Here, the high prio process is not getting scheduled while the low prio process executes. Hard to remove. 

__Feedback Queues__ Shorted job next is optimal but impossible. You can not predict when things are going to run, but you know that the past predicts the future. Short CPU bursts will imply an increased prio while long bursts decrease prio. This is determined by noting the activity in the past (does it use its entire time quantum or not?). The goal is to increase response time and throughput. 

__Lottery Scheduler__ Use statistics to determine bounds or how much CPU time to give to processes. Operates like a drawing lottery (basket full of balls with numbers) held at every moment you choose what to schedule. Give more balls to higher prio procs. You can game the system however by forking more. Here, to work around this, limit the balls. 

| Charactieristic | FCFS | SJN | RR | PrioQ | FQ | Lottery |
|:---------------:|:----:|:---:|:--:|:-----:|:--:|:-------:|
| CPU Usage       | High | High | High | High | High | High |
| Throughput      | Poor | Optimal | Fair | Fair | Good | Fair |
| Response Time   | Low  | Good | Bad | Good | Good | Good |
| Predictability  | Bad  | Bad | Bad | Bad  | Bad | Bad    | 
| Fairness        | Good | Not | Fair | Not | Not | Fair   | 

## Program 3 - Hot Potato
One process initiates listening to communications and another connects to it (someone has to dial the number). Messages sent is arbitrarily large. Limits will likely be hit. You do not have to use TCP/IP, but you can. This will be between systems, however, so you need to use a networking protocol. 

You will have to build a master and player program. The master will always listen on a port and wait for N players. Master tells players info. Then it throws out potato. Potato is thrown around until it goes off. When it goes off, hand it to the master. The players listen for initial info, potatoes, and shutdown commands. 

__Steps__
* Set up a network with 3N duplex links (N players and a link to your neighbors and master) 
* Do not do remote startup. Will run on any machine. 
* The potato is randomly launched. 
* __Potato__ Potato keeps track of hops while players decrement hops. On the last hop, sent to master. 
