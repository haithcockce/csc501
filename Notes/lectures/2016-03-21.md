# Memory Management part 2

### Review from the last lecture

##### Cache
You can have an address. The addresses in caches nearer to the CPU tend to be physical while virtual ones are further away. You have a tag and index in your address. The index indexes into the cache set. The set referenced is the amount of lines within the set associativity. 1 way (direct map) means the index indexes into the cache and you don't have to compare the tag. When dealing with N-way sets, you have to check the tag (even with fully associative). 
* Direct map is set associative with 1 way
* Set associative is n way with m/n sets (m being the size of the cache)
* Fully associative is essentially m way with 1 set. 

Why would you want direct mapped cache? Cheaper in time to compare tags. The cost is too great to make anything above 8 way associativity since it would take so long to compare. You trade off a decrease in the miss rate while you increase cost. The 2/4/8 way set associativity work except for TLB which is fully associative because the decrease in miss rate is worth it. This is because you are caching page addresses. Your locality is going to be at least 4096 B (4 KiB) apart since that is the size of each page. The cost of a cache miss is also huge. You have to go about page faulting. 

##### TLB
Why do we have them? Because the relative speeds of the CPU and memory  has changed. The cost to fetch something from Memory was very small compared to CPU speeds (around 1-5 cycles). Now the same cost is hundreds of cycles. Every instruction requires a translation which requires going to memory. So instead of going to memory, you cache it. About 1.3 translations per instructions since about 30% of all instructions are load and stores. 
* Typically have 256 entries and they are fully associative. 
* Address translations are per process. So a context switch invalidates the cache for the new process. This causes a TLB flush. So You end up having TLB misses until the cache warms up. You can include the PID in the TLB entries. So you can mix the lines of cache between the two processes. 
  * The advantage is in less misses on the TLB when switching but the disadvantage is that the tag is larger. 


### Paging

##### Paging example
Remember the Page table is indexed by the virtual page and stores the values of Page Table Entires. 

```
 VMA     PTE     PFN
.---.   .---.   .---.
| 0 |   | 3 |   | 1 |
|---|   |---|   |---|
| 1 |   | 0 |   | 3 | 
|---|   |---|   |---|
| 2 |   | 5 |   |   |
|---|   |---|   |---|
| 3 |   | 1 |   | 0 |
'---'   '---'   |---|
                |   |
                |---|
                | 2 |
                '---'
```
So the virtual memory typically is seen as contiguous, the index to the PTE is the VMA and the value therein would be the location of the physical page. The TLB is essentially this the same. You take the page number from the VMA to index into the TLB. The frame number associated with it is then used as the higher order address part. When you have a TLB miss, then you pull from the page table.  
On TLB miss (assuming a full TLB and a valid address in the PTE and the page is in memory):  
1. You evict an entry depending on the replacement policy. Typically this is LRU.  
2. Walk the page table to find the frame number.  
3. If not in memory, page fault, if so, populate it.  
When you have a valid address that is not in memory, you have a page fault. A page fault can occur because the page is requested and not yet allocated (new page), the page is swapped out, or the page is somewhere else on disk (major fault). 

### Page Table and PTE
What is a PTE? A data structure associated with the virtual page. Every virtual page has a PTE. The PTE has a bunch of metadata such as permissions (RWX), dirty, the kind of page (file, buffer, etc), etc.  

##### The Page Table structure
How big is the PT? Proportional to the VA space and invesrely proportional to page size. This makes sense, the larger the page size, the few addresses you need to hold. This is fine as VA space is pretty sparse anyway (remember, you are deailing with 32-bit or 64-bit address spaces. 64-bit is fucking huge).  

EX) Let's assume a PTE is 4 B, a 4 KiB page size and 32-bit address.  
* 2^12 page size (4 KiB)
* 2^20 pages (or 2^32 address width/2^12 page size)
* 2^20 pages * 2^2 PTE size = 4 MiB / PT / proc
 
EX) Let's assume a PTE is 4 B, 8 KiB page size, and 64-bit page size. 
* 2^13 page size
* 2^64 address width / 2^13 page size = 2^51 pages
* 2^51 pages * 2^2 PTE size = 2^53 or 8 Pebibytes / PT / proc

This can be simplified thankfully: 
* Linux does hierarchical paging. This is another level of indirection. You break into multiple levels of page tables. Take your virtual address, take the virtual page index of the address (the higest N order bits) and further break it up. The first part becomes the index into a list of page tables and then converted to a specific offset into the found PT. 
  * These PTs are lazily allocated. This way you don't allocate a jungus amount of memory. Most end up unallocated. 
  * This exploits the sparseness of VM address space. 
  * Can think of it like this: `Page Frame = PT2[PT1[virtual address part 1]][virtual address part2]`
  * 3 tier is more common as it causes less bits to check when performing look up and keeps the lists smaller. 
* The other idea is an inverted page table. Keep in mind that the current way is `PA = PT(VA)`, so the inverted page table is simply `VA = PT^-1(PA)`.
  * The size of the PT then is inveserly proportional to page size and proportional to physical memory. It is not necessarily physical memory / process since sometimes the physical memory is shared. 
  * The inverted page table has a hash function to help find the physical address from the VA. Not always will find it but will reduce cost. The hash could be simply the PID. Furthermore, since it has to be a hashtable, you will end up incurring an expensive lookup
  * The IBM family have hardware assists on this. 
   
### Demand Paging
Before VA, you had PA, and addresses were always resolved. With VM, the address could say that it could not be determined at that specific time of lookup. When demand paging (or page fault), the processor has said "I want to read this and it is not there". You only pull in pages you need, so you have less IO and less memory usage provided not all the pages are needed. Tradeoff: more comlexity, less IO/memory. The expectation is that you not exercise all the code. When a page is referenced: 
* If in memory, fetch it
* If not, go get it (page fault). This may require eviction if the memory is full. 
* If the page reference is invalid, then abort (SIGSEGV). 

##### Anatomy of a page fault
* Reference the page (via Load or whatever)
* If the reference is invalid, IE the PT does not have a valid entry (`valid == FALSE`) abort
* Otherwise (`valid == TRUE`) trap to the OS. 
* In OS handler:
  * the os finds the page on backing store (swapspace) or determine if it is new
  * bring page into physical frame, possibly evicting a page
  * update the PT and possibly the TLB and return from trap restarting the instruction. 
