# Low Bandwidth Network Filesystem

### Overview
* Motivated deduplication. Searched for duplicated data and helped lower bandwidth. 
    * For example, if you already have the file somewhere in your fs, cache hases of the files so you can simply associate a hash with a file. 

### Key Ideas
* Network file server for slow or wide-area networks
    * 
* This fs exploits similarities in files
    * caches data you already have
    * cache files that you don't already have data
* Allowed 90% decrease in bandwidth than traditional NFS-like filesystems

## LBFS

### LBFS: Working on slow networks
* Javascript allows a massive reduction to bandwidth since a lot more can be processed locally. 
* You can use local copies of things, but you would have to worry about update conflicts  

A file is now not a block, but instead a list of chunks identified by hash rather than location on disk (so a list of hashes). Clients and servers indexes files via hash. Client then simply sends/receives chunks 
* Tradeoffs:
    * Client side processing increased but amount sent reduced
    * Store more metadata for data rather than just data
    * Since transactions are compressed, you have to process more but again, less bandwidth

### Previous Work
* AFS has a callback model where clients are notified when a file is updated
* You can maintain fs consistency via leases. Not ideal but low overhead. Server leases file to client for file. You are guarenteed to have the newest data, but you have to request it and may block. Servers keep track of the leases until expiration. Clients have to wait for client expiration. Simple implementation but you have to constantly ask the server if the leases are short but you have to block if the leases are long. 
    * Tradeoffs are lot of time spent asking for leases or blocking waiting on leases, but simple implementation and low bw

### LBFS: Implementation 
Provides close-to-open consistency, or on closure, changes are made to file and server then receives data. Clients are assumed to have enough data to use. So new clients will suffer until the cache warms up. Likewise, workloads that do not benefit from cache (streaming for example) do not benefit. CLients will always try to reconstitute files using existing data chunks in the FS and cache instead of transmitting all the time  

How do you index and keep track of chunks? Redundency and consistency checking would be a continuum of tradeoff where as chunk size decreases, ease consistency increases and benefits increase, but so does overhead in maintenence of files. You can consider all possible chunk sizes within reasonable ranges. For the chunks, hash the chunks and compare based on chunks. You now have a ton of hashes however.  

To combat the massive amount of hashes, you consider only non-overlapping chunks. You can also use the data itself to determine chunk boundaries.  

The chunk database hashes with the first 64 bits of the SHA-1 hash. LBFS always recomputes the hash of any data chunk before using it. This simplifies crash recovery and recomputing the hash values can help prevent collisions (rehashing should you collide on the first 64 bits)

##### Reading a file
* Client checks cache. If not there, request server the set of hashes for the file. 
* The server breaks up file into chunks with an offset into the file and the count of chunks and sends it back to the client. 
* The client checks each hash for existence in hash db and requests the full chunk if the hash does not exist. 
* The server returns the chunks requested
* The client receives the chunks and calculates and stores the hashes for the chunks received
* This allows a file to be reconstructed  

##### Writing a file
* Make updates to file and then close. On closure, make a temporary file on client and server
* The client breaks up the file into chunks and sends the hashes to the server
* Server receives and updates hash db appropiately, requesting the corresponding chunks as necessary and sending OKs for the rest. 
* The client sends the chunks and commits the file locally and requests a commit to the server
* The server receives commits and commits the file. If everything went fine, then the servers sends an OK back
* The client receives the OK and closes the file at the OS layer. 

##### Protocol
* Based on NFS 3
* Adds exploit to interfile commonality and leases
* Traffic is compresssed over gzip

##### Rabin Fingerprint

You choose size of region window (48B) and a set of bits that identify a boundary (Rabin Fingerprint). If the data ever looks like the set (or the Rabin fingerprint) in the byte region, then the data with the set is a boundary. 
* In insertion: insert into a chunk. This means simply inserting the data and regenerating the hash. If the data inserted creates a boundary, then you will need to create multiple hashes. If you remove a boundary, simply regenerate a hash and remove the hash of the other chunk. 

### File Consistency
* Clients receive leases on file via RPC
* Clients receive file attr on request of a lease wherein the server grants the lease if it has expired and the client can check if the current cached version is the most updated version, requesting an update if the timestamps are different.
* No need for write leases since block all accesses when leases are out
* Limits concurrent updates reducing possible damage 

### Tradeoffs and Evaluations
* Fixed block vs boundary changes: boundaries can stay the same allowing reduction in duplication vs fixed blocks where insertion on fixed sizes will shift data across a ton of blocks. 
* SInce the chunk sizes are probabalistically determined, you can end up with very large or small chunks. Work around this with minimum and maximum chunk sizes. Limits overhead for small sizes and prevents tons of RPC requests for a single chunk for large chunks
* Except for brand new files, the overlap on files is large, 55% - 90% and chunk sizes are on average of 4-8 KiB. 
    * Sometimes the files themselves are smaller than the lower limit
