# Working set

### The Working Set Model for Program Behavior
Presented some ideas such as a model of computing, how the past predicts the future, and the problem of global resource allocation. How do you go about allocating them and what is needed and how much? It's a balancing argument.  
What motivates resource mgmt? You need to coherently allocate resources and memory in a unified manner. But there is also no adequate model of program behavior. 

##### System Solution
You can not rely on outside help such as a copmiler or programmer. We may be able to rely on compilers nowadays though.  
What motivated this doubt? 
* Resources are transparently multiplexed. You don't interact with resource allocation directly, the system does that for you. 
* The user has an illusion of a virtual computer (abstract user from VM). You can also argue that the user may not be altruistic and hog resources starving other processes.  

The model in use is a single processor with an "infinite" one-level flat virtual memory. IE, we have all the memory we want/need. This already exists today as VM and swap space, so this is not far fetched. So why can the user not provide good dynamic info? The user is using code they did not write, so the user may not be extremely knowledgable at that level. You could learn it, but it would be a great deal of unnecessary work. Next, what exactly is the user supposed to say? Is the naive user going to get resources unfairly? Furthermore, the advise is local to the process and not gloablly applicable across all systems. Compilers can not be trust either (but this is arcane).  

##### Goal - Minimize paging traffic (to/from IO) to reduce overhead and unnecessary congestion

##### Model Overview
Stages: 
* Paging in
    * This is essentially demand paging and purely reactive 
    * You can not really consider much else
* Paging out
    * This is the main problem, what do you page out? 
    * Best option is to page out the page least likely to be paged back in. The optimal is further future reference in time. Theoretically, page out the one infinitely far from now in time. This requires prediction however. 
    * This is page replacement policy.

##### Page Replacement Policies
Most of these are some form of LRU. 
* Random
   * This is simple but silly 
* FIFO
   * Simple
   * Downside is that you always hit worst case scenario with temporal locality. 
* LRU
   *  Simple to describe, but hard to implement. You have to touch with a timestamp everytime you touch it. FIFO you have to update only once (when paged in). LRU requires timestamp touches with every touch. Thankfully, there is hardware support. 
   *  No one does pure LRU due to overhead. So an approximation is done. 
   *  Why is nearly everything LRU? Because the past predicts the future. This is not always true, but we have nothing else and simply take the pentalty for being wrong. 
   *  Works fine for data streams and temporal locality
* ATLAS loop detection method. 
   * Tries to predict the future more intelligently by detecting loops, but no always a great. Furthermore, stacks and recursive algorithms (loops operate as such: 1, 2, 3, 1, 2, ... while recursive operate like 1, 2, 3, 4, 3, 2, 1). 

### Working Set
_Working Set_ The smallest collection of data that must be present to assure efficient execution. 
* Efficient is subjective here, but you could indicate this as the adequate amount of memory for an execution time quantum. 
* `W(t, tau): a time t, we have the working set available. From tau to t, we gathered the data referenced to satisfy the working set. `
* Here `tau` is the time required to put the system sufficiently efficient. 
* `w(t, tau` is the number of pages in `W(t, tau)` or the number of pages in the working the set
   * So `tau` is something that we can assign and require for the system to meet to make everything efficient. 
* Assumptions:
   * Assume the pages for the working set is continuously in memory. Or keep the working set in memory. 
   * Evict pages that no longer belong in the working set
      * Overlapping intervals of tau to t of different sets will have the same pages in the set, but non overlapping sections may not have the same pages
   * Process is not interrupted
   * No data sharing between processes (this is not true). 
##### Properies
* Size: 
   * `w(t, 0) = 0` boundary condition
   * `w(t, tau)` monotonically inceases as tau increases (non-decreasing)
   * `W(t, 2tau) = W(t, tau) UNIONED W(t-tau, tau)`
   * This implies that `w(tau, 2tau) <= w(t, tau) + w(t-tau, tau)`
   * This is bounded however. If an applicatino references x amount of pages at most, then as tau increases, the function begins to platau
* The past predicts the future (Temporal Locality)
   * After some time, the past is less predictable
   * Formally, for some `alpha < tau`
      * the probability forthe intersection of a working set for the immediate future for a small `alpha` and the current working set being the null set is very small. 
      * For sufficiently large `alpha`, the current working set is not a great predictor for the future working set.
* Reentry Rate
   * As `tau` decreases, the working set size decreases
   * With this the probability of any page being the working set decreases. Conversly page miss rate increases. 
* `Tau` Sensativity
   * How to pisk an appropriate tau as the reentry rate varies with tau. 
      * Depends on the memory size, latency to memory, process requirements, and number of processes
      * When tau is too small, pages are evicted prematruely while too large means memory is not efficiently used. 
   * You may be able to capture a tau in hardware but this is impractical. The software solution could include sampling the PTE

##### Conclustion
* The paper started with working sets and processes. These are the manifestations of computation. So the demands are captured in the processes. The working set model is ideal, but not practical. Nothing practical has been built?
