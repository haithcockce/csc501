# Memory

### Overview
A Von Neumann machine is a stored program computer where instructions run on a CPU and fetched from Memory.  

### Memory Hierarchy
Lowest, smallest, closest to CPUs  
* Registers
* Caches (L1 - L3)
* Main Memory
* VM 

### i7 Case Study
1 cycle is the time to complete for a single instruction. Timings for memory access:  
* Registers: 0 cycles
* L1 hit: ~4 cycles
* L2 hit: ~10 cycles
* L3 hit (unshared): ~40 cycles
* L3 hit (shared, unmodified): ~64 cycles
* L3 hit (shared and modified): ~75 cycles
* L3 hit (remote): ~100-300 cycles
* DRAM hit: ~200 cycles
* Remote DRAM: ~350 cycles
  * L1 and L2 are private to the core
  * L3 is shared across all the cores on the same physical chip
  * Multiple L3s can exist across multiple chips
  * A local L3 can have a particular value and that value be the same across all L3, but a remote L1/2 could have an update pending a flush to L3, so you would have to wait for that to flush if you have a remote hit
  * Size (including actual width) increases as you walk down the hierarchy, but price goes down. Perfomance then has an inverse relationship to latency. 

### What is Virtual Memory 
It is an abstraction that provides an address space. It is the set of all possible addresses (both physical and logical) that are visible to a process.  
* It is a literal set, so no unique values and not guarenteed to be contiguious. 
* Some addresses are not visible to the process
* It is perprocess, so each has its own unique address space
* Some of the physical memory is accessible to the process
A _logical (virtual) address_ is the address generated by the program/processor while a _physical address_ is the address presented to the memory bus which goes throughthe memory higherarchy (caches and DRAM). The closer you are to registers, the more likley you will store VA whereas closer to DRAM you will likely store PA.  

Perl of wisdom: _Separate mechanism from policy_ What is VM in this manner? The mechanism is the indirection.  
* You can share pages when the same two VM pages have the same content, so they can map to the same PM page. 
* Also allow copy on write. You can share writeable pages, but the write is a lazy copy meaning that we copy only when a change occurs in one process. 
* You are not limited to implementation; you do not have to write the same application for large or small memory 

Only a part of memory needs to be loaded when running. The entire program can be loaded into VM while only the parts touched will be brought into PM (minor fault). 
Virtual Memory - Programs would access memory directly
* Difficult to perform in a general manner
* VM solved two things: allowed location independence, and overcommit (more locations)

##### Components of VM systems
* _MMU_ Memory Mgmt Unit. Comprised of the following: 
  * This is hardware driven and contains a Translation Lookaside Buffer (TLB) which together, the MMU validates VA. If the VA is not bound to a PA, then you fault. It also checks permissions. It is accessed only via protected instructions within kernel mode. The OS uses these instructions to manipulate the TLB for example when switching contexts (this forces a TLB flush). This also returns a physical address when mapping a VA to PA. When the address is invalid, it traps to the OS (SIGSEGV in particular). Can think of it like this: `PA = f(VA)`. 
  * The MMU controls cache
    * There is fully associative, set associative, and direct associative. Direct associative means an address maps directly to a line of cache. Set addociative means an address maps to the start of a set, and then we pull in N lines of cache. Part of the address then determines which line in the pulled set is the one we want. Fully associative means an address pulls all lines of cache and then determines which line it needs. 
    * Fully associative has a lot of latency since you have to do a tag look up everytime. 
    * Direct associative is expensive in price since it is best to have a lot more. 
    * Set associative is a nice balance between the two. 
  * along with DRAM (main memory). Main memory is not always uniform (NUMA). When dealing with PM, the OS interacts only with a page (aka, page frame) at a time. 
  * Also controls swap space. This is typically on disk. This typically stores dirty pages (page cache and Anonymous memory). 
  * Page table (PT) or a map of VA to PA. In particular it is a lookup: `Entry = PT[VA]`. This will actually also have not just the PA but also if the page is dirty etc. When the CPU issues an address, the address is decomposed into a logical address and an offset into the PT. At the offset in the PT, you get the offset in DRAM and prepend this to the phyiscal address translated from the logical address. 

```
|-----------------------------------------------------------------------| Width of address (32 or 64 bits)
.-----------------------------------------------------------------------.
|         Virtual                             |     Physical            |
'-----------------------------------------------------------------------'
                 |                                         |
                 v                                         |
             .-------.                                     '-------.
             |       |                                             v
             |  PT   |                      .--------------------------------.
             |       |--------------------> |  PT entry           | Physical |
             '-------'                      '--------------------------------'
                 
```


    * The TLB on average performs 1.5 memory references / translations per instruction (for C)
    * 30% of instructions are Load/Stores
    * The TLB is a cahe for these PA -> VA mappings (it does a few more things but this is primarily what it does)
    * Why do caching addresses work? We are exploiting locality (temporal and spatial). However, with TLB, we are exploiting only the temporal locality since we are pulling in pages from memory and we are addressing them with the TLB. 
    * TLB tend to be fully associative with 64-256 entries because we have to be able to address all of memory. You can not assume PA is close to VA. So the TLB reach is equal to the number of entries * sizes of the pages. So if you have 4 KiB pages and 64 entries, you get only 256 pages easily addressible. 
RISC - although nothing really means its reduced any more, the RISC means all operations are performed in registers instead of CISC which allow interaction with memory directly.
