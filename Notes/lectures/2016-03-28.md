# Cache pt2

### Review:
In cache timings, you get about 4 cycles to access L1 cache at 64 KiB, 10 cycles for L2 at 256 KiB, and 40 cycles at L3 and 2MiB. This means you have to fill the pipeline while getting instructions. You evenhave multiple issues (2 instructions per cycles). And approximately 30% of all instrcutions are loads/stores. Local DRAM requires about 60 ns (cycles aren't important), but you have 4 or more GiB. Remote DRAM (NUMA miss essentially) 100ns which can have tons of memory. Local disks can be about 10 ms (not nanoseconds) but going up to terrabytes. Remote (internet) can be 200 ms but is massive (exabytes). 
* Why does cache work? 
* Exploits spatial and temporal locality. 
* _Zipf Distribution_ ~k^-a. Common distrubtion in nature. IE a few things are way more popular than the majority of the others. Popularity! In this case, the rich get richer and the poor get poorer. The tail is extremely large. This also goes for page replacement policies. 
* Random replacement is unpredictable but on average will not predict the worst, but also on average will never predict the best. 
* FIFO: bad for common workloads, but ok in the sense that pages spend about the same amount of time in cache. If you have a workload that counters the Zipf law, then this may work.
* LRU: Best replacement policy as it uses the past to predict the future. Bad in some common cases as it requires scanning pages which means invalidating cache
* Lease Frequently Used: based on zipf distro. When LRU visits an infrequent page, it is treated as imported, but LFU does not. This may be bad for termporal locality (IE a common page is not necessary for a while and will not age out, wasting space). 
  * These two are the best. 
_Belady's Anomaly_ As you increase the cache size, you should increase hit rate (and generally this is true), but for many common algorithms like LRU for replacement policies, this isn't the case. LRU, LFU, and FIFO can suffer from the anomoly. 

### Memory Consistency Models
_Sequential Consistency_ Example of a model, or the result of any execution as if all the operations across all CPUs were in some sequential order. Meaning in any processor, you have A then B then C then D while on another you have 1, 2, 3, and 4. So then you could have any partial order obeyed, or something like 1, A, 2, B, 3, C, 4, D or A, B, 1, 2, 3, C, D, 4, etc. Another definition sayd a read returns the most recent write (pipelining?).  
With shared memory multiprocessors (SMP), each processor has cache, but a processor may need to get cache from another processor. What happens? Options: 
* Prevent it
* Delay the request and flush
* Or if just a read and we are not modifying it, just take it!
* You can also just have a weaker consistency model. You may have to refresh multiple times on your browser for examples

WHen performing memory access, you have either private or shared memory (like shm). Within shared, you have non-competing and competing. Within competing, you have non-sync or syncronous. On sync, you can have release (lazy) or acquire (greedy). In acquire you have exclusive or non-exclusive. 

* A MCM (Memory Consistency Model) is restrictive. Meaning you may not be able to read one thing before the other so the second thing blocks on the first. This may be useless if the blocking thing has no relationship with the new thing. A weaker model would have more histories. Certain things should be agreed on but not all access need to be managed. Sequential Consistency restricts only on location or time. 

Atmoic Consistency
* Strong consistency model
* All operations take place in an operation interva, the intervals are non-overlapping, and the idea is that reads occur at the beginning and writes at an end (SEMAPHORES ARE LIKE THIS I think...). Unimplementable due to cost of time to guarentee. 

### Progam 4 - RAM Filesystem using FUSE (Fs in USErspace)
* malloc your fs
